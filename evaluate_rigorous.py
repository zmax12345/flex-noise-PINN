import torch
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from dataset import SpeckleFlowDataset
from model import SpecklePINN
import os

# ================= é…ç½® =================
CONFIG = {
    # ğŸ”¥ 1. è¿™é‡Œå¿…é¡»æ”¹æˆä½ æ–°æ•°æ®çš„è·¯å¾„ï¼Œå¹¶ä¸” Key è¦å’Œ dataset.py é‡Œçš„ elif å¯¹åº”ï¼
    'roots': {
        # ä¾‹å¦‚ï¼šä½ çš„ dataset.py é‡Œå†™çš„æ˜¯ elif 'new_experiment' in group_name...
        'group_2.3': '/data/zm/2026.1.12_testdata/2.3/'
    },

    'window_size_us': 150000,
    'step_size_us': 50000,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',

    # æŒ‡å‘ä½ åˆšæ‰ç”¨å…¨é‡æ•°æ®è®­ç»ƒå‡ºæ¥çš„é‚£ä¸ªæ–°æ¨¡å‹
    'model_path': '/data/zm/2026.1.12_testdata/2.5PINN_Result/model_train/best_model.pth',

    # ğŸ”¥ 2. è¿™é‡Œè®¾ä¸ºç©ºåˆ—è¡¨ []
    # æ„æ€æ˜¯ï¼š"ä¸è¦è¿‡æ»¤ï¼ŒæŠŠæ–°æ–‡ä»¶å¤¹é‡Œçš„æ‰€æœ‰æµé€Ÿæ–‡ä»¶éƒ½æµ‹ä¸€é"
    # å¦‚æœä½ åªå¡« [0.8, 1.8], å®ƒå°±åªåŠ è½½è¿™ä¸¤ä¸ªæµé€Ÿçš„æ–‡ä»¶ï¼Œå…¶ä»–çš„ä¼šè·³è¿‡ã€‚
    'holdout_flows': []
}


def evaluate_rigorous():
    print("Loading EVALUATION dataset (New Data)...")

    # 3. è¿™é‡Œ mode='train' è¿˜æ˜¯ 'val' éƒ½å¯ä»¥ï¼Œå› ä¸º holdout_flows æ˜¯ç©ºçš„
    # ä½†ä¸ºäº†é€»è¾‘ä¸€è‡´ï¼Œæ—¢ç„¶æ˜¯åšéªŒè¯ï¼Œç”¨ mode='val' ä¸” holdout=[] (å…¨ä¸ä¿ç•™=å…¨éƒ½è¦)
    # ç­‰ç­‰ï¼Œdataset.py é‡Œçš„é€»è¾‘æ˜¯ï¼š
    # if mode == 'val' and not is_holdout: continue
    # å¦‚æœ holdout_flows ä¸ºç©ºï¼Œis_holdout æ°¸è¿œæ˜¯ Falseï¼Œé‚£ 'val' æ¨¡å¼ä¸‹ä»€ä¹ˆéƒ½è¯»ä¸åˆ°ï¼

    # ğŸ”¥ å¿…é¡»ç”¨ mode='train' é…åˆ holdout_flows=[]
    # æ‰èƒ½éª—è¿‡ dataset.py è¯»å–æ‰€æœ‰æ–‡ä»¶
    val_ds = SpeckleFlowDataset(CONFIG['roots'], mode='train',
                                holdout_flows=CONFIG['holdout_flows'],
                                window_size_us=CONFIG['window_size_us'],
                                step_size_us=CONFIG['step_size_us'])

    # ä¸æ‰“ä¹±ï¼ŒæŒ‰é¡ºåºå–
    val_loader = DataLoader(val_ds, batch_size=1, shuffle=False)

    model = SpecklePINN().to(CONFIG['device'])
    if not os.path.exists(CONFIG['model_path']):
        print(f"Model not found at {CONFIG['model_path']}")
        return
    model.load_state_dict(torch.load(CONFIG['model_path']))
    model.eval()

    results = {}

    print("Running Inference on New Data...")
    with torch.no_grad():
        for batch in val_loader:
            g2_obs = batch['g2_curve'].to(CONFIG['device']).float()
            aux = batch['aux_input'].to(CONFIG['device']).float()
            v_label = batch['flow_label'].item()
            m_val = batch['k_factor'].to(CONFIG['device']).float()

            out = model(g2_obs, aux, m_val)
            v_pred = out['v_pred'].item()
            g2_hat = out['g2_hat'].cpu().numpy()[0]
            g2_obs = g2_obs.cpu().numpy()[0]

            if v_label not in results:
                results[v_label] = {'preds': [], 'errs': [], 'curves': []}

            results[v_label]['preds'].append(v_pred)
            results[v_label]['errs'].append(abs(v_pred - v_label))
            # å­˜å‡ ä¸ªæ›²çº¿ç”»å›¾ç”¨
            if len(results[v_label]['curves']) < 2:
                results[v_label]['curves'].append((g2_obs, g2_hat, v_pred))

    # --- ç»˜å›¾ä¸ç»Ÿè®¡ ---
    unique_flows = sorted(results.keys())
    if len(unique_flows) == 0:
        print("No samples found! Check dataset path and keys.")
        return

    # åŠ¨æ€è°ƒæ•´ç”»å¸ƒå¤§å°
    fig, axes = plt.subplots(len(unique_flows), 2, figsize=(12, 4 * len(unique_flows)))
    if len(unique_flows) == 1: axes = axes.reshape(1, -1)

    print("\n========= æ–°æ•°æ®æ³›åŒ–æµ‹è¯•æŠ¥å‘Š =========")

    for i, flow in enumerate(unique_flows):
        data = results[flow]
        mean_mae = np.mean(data['errs'])
        mean_pred = np.mean(data['preds'])
        std_pred = np.std(data['preds'])

        print(f"æµé€Ÿ: {flow:.2f} mm/s")
        print(f"   -> å¹³å‡é¢„æµ‹: {mean_pred:.2f} Â± {std_pred:.2f}")
        print(f"   -> MAE: {mean_mae:.4f}")
        if flow != 0:
            print(f"   -> ç›¸å¯¹è¯¯å·®: {(mean_mae / flow) * 100:.2f}%")

        # ç”»å·¦å›¾ï¼šè¯¯å·®åˆ†å¸ƒæ•£ç‚¹
        ax_scatter = axes[i, 0] if len(unique_flows) > 1 else axes[0]
        ax_scatter.hist(data['preds'], bins=20, alpha=0.7, color='green', label='Preds')
        ax_scatter.axvline(flow, color='red', linestyle='--', linewidth=2, label='Ground Truth')
        ax_scatter.set_title(f"Label v={flow:.2f} | MAE={mean_mae:.2f}")
        ax_scatter.legend()

        # ç”»å³å›¾ï¼šæ›²çº¿æ‹Ÿåˆæƒ…å†µ
        ax_curve = axes[i, 1] if len(unique_flows) > 1 else axes[1]
        if len(data['curves']) > 0:
            obs, hat, pred_v = data['curves'][0]
            ax_curve.plot(obs, 'b.', alpha=0.5, label='Observed')
            ax_curve.plot(hat, 'r-', linewidth=2, label=f'PINN (v={pred_v:.2f})')
            ax_curve.set_title(f"Curve Fitting (Sample)")
            ax_curve.legend()
            ax_curve.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('/data/zm/2026.1.12_testdata/2.3/generalization_test_result.png')
    print("====================================")
    print("ç»“æœå›¾å·²ä¿å­˜è‡³ generalization_test_result.png")


if __name__ == "__main__":
    evaluate_rigorous()