import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
import os
from tqdm import tqdm

from dataset import SpeckleFlowDataset
from model import SpecklePINN

# ================= é…ç½® =================
CONFIG = {
    # 1. è®­ç»ƒé›†è·¯å¾„ (åŸæœ¬çš„é‚£ä¸¤ç»„)
    'train_roots': {
        'group_680W': '/data/zm/2026.1.12_testdata/1.15_150_680W/',
        'group_gaoyuzhi': '/data/zm/2026.1.12_testdata/gaoyuzhi/'
    },

    # 2. éªŒè¯é›†è·¯å¾„ (ä½ çš„æ–°æ•°æ®æ”¾åœ¨è¿™é‡Œ!)
    # è¯·ä¿®æ”¹ä¸ºä½ æ–°æ•°æ®çš„çœŸå®è·¯å¾„
    'val_roots': {
        'group_580': '/data/zm/2026.1.12_testdata/1.15_150_580W/'
    },

    'window_size_us': 400000,
    'step_size_us': 50000,
    'batch_size': 64,
    'lr': 1e-3,
    'epochs': 50,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu',
    'lambda_flow': 1.0,
    'lambda_fit': 10.0,  # å¦‚æœç”¨äº†åŠ æƒï¼Œè¿™ä¸ªå¯ä»¥é€‚å½“é™ä½ï¼Œæ¯”å¦‚ 1.0
    'save_dir': '/data/zm/2026.1.12_testdata/2.5PINN_Result/model_train',  # æ¢ä¸ªæ–‡ä»¶å¤¹å­˜æƒé‡ï¼Œåˆ«è¦†ç›–äº†ä¹‹å‰çš„
}


def main():
    os.makedirs(CONFIG['save_dir'], exist_ok=True)

    # --- 1. åŠ è½½è®­ç»ƒé›† (å…¨é‡) ---
    print("Loading TRAIN dataset (All Old Data)...")
    # holdout_flows è®¾ä¸ºç©º []ï¼Œè¡¨ç¤ºä¸ä¿ç•™ï¼Œå…¨éƒ¨ç”¨äºè®­ç»ƒ
    train_ds = SpeckleFlowDataset(
        data_roots=CONFIG['train_roots'],
        mode='train',
        holdout_flows=[],  # <--- å…³é”®ä¿®æ”¹ï¼šç©ºåˆ—è¡¨ = å…¨é‡è®­ç»ƒ
        window_size_us=CONFIG['window_size_us'],
        step_size_us=CONFIG['step_size_us']
    )

    # --- 2. åŠ è½½éªŒè¯é›† (æ–°æ•°æ®) ---
    print("Loading VAL dataset (New Unseen Data)...")
    # è¿™é‡Œçš„ mode æ— æ‰€è°“äº†ï¼Œå› ä¸º holdout_flows ä¸ºç©ºï¼Œ
    # ä½†ä¸ºäº†é€»è¾‘é€šé¡ºï¼Œæˆ‘ä»¬è¿˜æ˜¯è®¾ä¸º 'val'ï¼Œä¸” holdout è®¾ä¸ºç©ºï¼ˆè¡¨ç¤ºè¯¥æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡ä»¶éƒ½æ˜¯éªŒè¯é›†ï¼‰
    # æ³¨æ„ï¼šdataset.py çš„é€»è¾‘æ˜¯ï¼š
    # if mode='val' and not is_holdout: continue
    # æ‰€ä»¥ä¸ºäº†è®©å®ƒè¯»å–æ‰€æœ‰æ–°æ–‡ä»¶ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå°æŠ€å·§ï¼š
    # æŠŠ holdout_flows è®¾ä¸º None æˆ–è€…ä¸€ä¸ªç‰¹æ®Šæ ‡è®°ï¼Ÿ
    # ä¸ï¼Œæœ€ç®€å•çš„åŠæ³•æ˜¯ç›´æ¥æ”¹ä¸€ä¸‹ dataset.py è®©å®ƒæ›´çµæ´»ï¼Œ
    # æˆ–è€…ç›´æ¥ç”¨ mode='train' (å› ä¸º train æ¨¡å¼ä¸‹å¦‚æœä¸åŒ¹é… holdout å°±ä¼šè¯»å–)ï¼Œ
    # ä½†è¿™å¬èµ·æ¥å¾ˆæ€ªã€‚

    # ğŸ’¡ æœ€ä¼˜è§£ï¼šç¨å¾®æ”¹ä¸€ä¸‹ dataset.py çš„é€»è¾‘ï¼Œæˆ–è€…ç®€å•ç²—æš´åœ°ï¼š
    # åœ¨ä¸‹é¢è°ƒç”¨æ—¶ï¼ŒæŠŠ mode='train' ä¼ ç»™éªŒè¯é›† (æ„æ€æ˜¯"è¯»å–æ‰€æœ‰éæ’é™¤æ–‡ä»¶")
    # å› ä¸ºæˆ‘ä»¬çš„ val_roots é‡Œå…¨æ˜¯æ–°æ•°æ®ï¼Œæˆ‘ä»¬å¸Œæœ›å…¨è¯»è¿›æ¥ï¼Œä¸”æ²¡æœ‰ä»»ä½•æ’é™¤é¡¹ã€‚
    val_ds = SpeckleFlowDataset(
        data_roots=CONFIG['val_roots'],
        mode='train',  # è¿™é‡Œç”¨ 'train' æ˜¯ä¸ºäº†éª—è¿‡ dataset.py è®©å®ƒè¯»å–æ‰€æœ‰æ–‡ä»¶
        holdout_flows=[],  # ä¸æ’é™¤ä»»ä½•æ–‡ä»¶
        window_size_us=CONFIG['window_size_us'],
        step_size_us=CONFIG['step_size_us']
    )

    train_loader = DataLoader(train_ds, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=4)
    val_loader = DataLoader(val_ds, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=4)

    print(f"Data split: Train={len(train_ds)} slices, Val (New)={len(val_ds)} slices")

    # 2. æ¨¡å‹
    model = SpecklePINN().to(CONFIG['device'])
    optimizer = optim.Adam(model.parameters(), lr=CONFIG['lr'])
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)

    # 3. è®­ç»ƒ
    print("Start Training (Rigorous Physics Mode)...")
    history = {'train_loss': [], 'val_loss': []}

    # å®šä¹‰ Fit Loss çš„æƒé‡ (å¯é€‰ï¼šç»™å¤´éƒ¨æ›´é«˜æƒé‡)
    # æ—¢ç„¶å½’ä¸€åŒ–ä¿®å¥½äº†ï¼Œæš‚æ—¶ç”¨å‡åŒ€æƒé‡
    # Fit loss æƒé‡ï¼šå¼ºè°ƒæ—©æœŸä¸‹é™æ®µï¼ˆä½ å…³å¿ƒçš„å‰ 1ms / 5msï¼‰
    tau_us = (model.tau_grid.detach().cpu().numpy() * 1e6).astype(np.float32)
    w = np.ones_like(tau_us, dtype=np.float32)
    w[tau_us <= 1000.0] = 5.0
    w[(tau_us > 1000.0) & (tau_us <= 5000.0)] = 2.0
    w[tau_us > 100000.0] = 1.5  # ğŸ”¥ ç»™ 100ms ä»¥åç¨å¾®åŠ ä¸€ç‚¹ç‚¹æƒé‡ï¼Œå¼ºè¿«æ¨¡å‹çœ‹æ…¢é€Ÿè¡°å‡
    # å½’ä¸€åŒ–ï¼šè®©å¹³å‡æƒé‡ä¸º 1ï¼Œé¿å…ç­‰æ•ˆ lambda_fit çªå˜
    w = w / (np.mean(w) + 1e-9)
    fit_weights = torch.from_numpy(w).to(CONFIG['device'])

    for epoch in range(CONFIG['epochs']):
        model.train()
        total_loss = 0
        valid_batches = 0

        pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{CONFIG['epochs']}", unit="batch")

        for batch in pbar:
            g2_obs = batch['g2_curve'].to(CONFIG['device']).float()
            aux = batch['aux_input'].to(CONFIG['device']).float()
            v_label = batch['flow_label'].to(CONFIG['device']).float()
            m_val = batch['k_factor'].to(CONFIG['device']).float()

            optimizer.zero_grad()

            out = model(g2_obs, aux, m_val)

            # Loss è®¡ç®—
            g2_hat = out['g2_hat']

            # Fit Loss
            loss_fit = torch.mean(fit_weights * (g2_hat - g2_obs) ** 2)

            # Flow Loss
            v_pred = out['v_pred']
            loss_flow = torch.mean((v_pred - v_label) ** 2)

            loss = CONFIG['lambda_fit'] * loss_fit + CONFIG['lambda_flow'] * loss_flow

            if torch.isnan(loss):
                continue

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            total_loss += loss.item()
            valid_batches += 1

            pbar.set_postfix({
                'L': f"{loss.item():.2f}",
                'Fit': f"{loss_fit.item():.2f}",
                'Flow': f"{loss_flow.item():.2f}"
            })

        avg_loss = total_loss / valid_batches if valid_batches > 0 else 0.0
        history['train_loss'].append(avg_loss)

        # === éªŒè¯ ===
        model.eval()
        val_loss_sum = 0
        val_count = 0

        with torch.no_grad():
            for batch in val_loader:
                g2_obs = batch['g2_curve'].to(CONFIG['device']).float()
                aux = batch['aux_input'].to(CONFIG['device']).float()
                v_label = batch['flow_label'].to(CONFIG['device']).float()
                m_val = batch['k_factor'].to(CONFIG['device']).float()

                out = model(g2_obs, aux, m_val)
                v_err = torch.abs(out['v_pred'] - v_label).mean()

                val_loss_sum += v_err.item()
                val_count += 1

        avg_val_mae = val_loss_sum / val_count if val_count > 0 else 0.0
        history['val_loss'].append(avg_val_mae)

        scheduler.step(avg_val_mae)

        print(f"Epoch {epoch + 1} | Train Loss: {avg_loss:.4f} | Val MAE (Unseen Flows): {avg_val_mae:.4f}")

        if epoch > 0 and avg_val_mae < min(history['val_loss'][:-1]):
            torch.save(model.state_dict(), os.path.join(CONFIG['save_dir'], 'best_model.pth'))

    plt.figure(figsize=(10, 5))
    plt.plot(history['train_loss'], label='Train Loss')
    plt.plot(history['val_loss'], label='Val MAE (Holdout)')
    plt.legend()
    plt.savefig(os.path.join(CONFIG['save_dir'], 'training_result.png'))
    print("Rigorous Training Complete.")


if __name__ == "__main__":
    main()